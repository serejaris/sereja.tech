---
title: "Map-Reduce для YouTube метаданных через AI субагентов"
description: "Как я генерирую точные заголовки и описания из транскрипта видео через параллельных агентов — без галлюцинаций и с реальными сущностями."
date: 2026-02-02
tags: ["agents", "youtube"]
youtube_id: "AHIaQKQqfRM"
---

Записал полуторачасовой стрим про OpenClaw:

{{< youtube AHIaQKQqfRM >}}

Загрузил на YouTube. Смотрю: пустое описание, заголовок "Stream 2026-01-28", нет timestamps.

Можно придумать метаданные самому, но через 106 минут записи я уже не помню, что говорил в 23-й минуте. А LLM может просто выдумать — галлюцинация.

Нужны метаданные из реального контента. Из транскрипта.

## Проблема длинного контекста

Транскрипт видео на 106 минут — это 3968 сегментов. Скормить всё одной моделью? Формально можно — Claude Opus 4.5 берёт до 200K токенов.

Но есть проблема: [Lost in the Middle](https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf). LLM теряют 30-85% точности в середине длинного контекста. Первые факты помнят, последние помнят, а середина — размазанная каша.

Проверял на своём стриме. Попросил модель найти факт из 45-й минуты в single-pass анализе — выдумала деталь, которую я не говорил.

Решение — [Map-Reduce](https://gonzalo123.com/2026/01/12/using-map-reduce-to-process-large-documents-with-ai-agents-and-python/). Разбить транскрипт на куски, обработать параллельно, потом объединить.

## Pipeline из 4 стадий

```
[Транскрипт 3968 сегментов]
         │
         ▼
┌────────────────────────────┐
│  1. Chunking               │  Семантическая разбивка
│     → 14 чанков по 5-10мин │
└────────────────────────────┘
         │
         ▼
┌────────────────────────────┐
│  2. Extraction             │  Параллельные субагенты
│     14 × Haiku (Map)       │
│     → 232 entities         │
└────────────────────────────┘
         │
         ▼
┌────────────────────────────┐
│  3. Aggregation            │  Объединение (Reduce)
│     1 × Sonnet             │
│     → 7 глав, 20 entities  │
└────────────────────────────┘
         │
         ▼
┌────────────────────────────┐
│  4. Metadata               │  Финальная генерация
│     1 × Sonnet             │
│     → titles, description  │
└────────────────────────────┘
```

**Chunking** — разбил транскрипт на 14 семантических кусков по 5-10 минут. Не по количеству токенов, а по смыслу: один чанк = одна тема.

**Extraction** — запустил 14 параллельных субагентов на Haiku. Каждый обработал свой чанк и вытащил упоминаемые инструменты, концепции, проблемы. Получил 232 сущности. В моём случае стадия заняла 30 секунд.

**Aggregation** — Sonnet взял все 232 сущности, схлопнул дубликаты, ранжировал по частоте упоминаний. Структурировал в 7 глав, оставил 20 ключевых сущностей.

**Metadata** — Sonnet сгенерировал три варианта заголовка (60-80 символов), описание с терминами из видео, 14 timestamps, теги.

## Промпт для Extraction

{{< callout type="insight" >}}
Проанализируй этот фрагмент транскрипта. Извлеки:
- Упоминаемые инструменты (OpenClaw, MoltBot, Claude Code)
- Обсуждаемые концепции (AI-агенты, промпт-инжиниринг)
- Проблемы и решения

Считай только явные упоминания. Не выдумывай. Верни JSON с entities и их count.
{{< /callout >}}

Каждый из 14 агентов получил такой промпт. Работали параллельно.

## Результат

Топ-3 сущности из моего стрима:
- **OpenClaw** — 78 упоминаний
- **Claude** — 62 упоминания
- **Telegram** — 42 упоминания

Три варианта заголовка, 14 timestamps с названиями глав, описание с реальными терминами из видео.

## Почему это важно для YouTube

[70% YouTube трафика](https://graphaize.com/youtube-seo-in-2026-guide-to-ranking-and-growth/) приходит из рекомендаций. Алгоритм оценивает relevance — насколько метаданные соответствуют контенту.

[Транскрипт индексируется поисковиками](https://youtubetranscripts.org/blog/youtube-seo-with-transcripts). 10-минутное видео = примерно 1500 слов indexable content (при обычном темпе речи). Если заголовок упоминает "OpenClaw", а в транскрипте слово "OpenClaw" встречается 78 раз — это signal для алгоритма.

Map-Reduce даёт метаданные из реального контента. Fact-based extraction вместо галлюцинаций модели или моей памяти через полтора часа.

## Technical takeaway

Map-Reduce избегает две проблемы длинного контекста:
- **Quadratic complexity** — внимание O(n²) растёт быстрее контекста
- **Capacity overflow** — середина документа теряется

Параллельные агенты на Haiku быстрее и дешевле одного вызова Opus. Sonnet на финальной стадии объединяет уже структурированные данные, не сырой текст.

Использовал Claude Code для всего pipeline — от чанкинга до генерации. Транскрипт из ai-whisper проекта.

---

Видео со сгенерированными метаданными: [youtube.com/@serajaris](https://youtube.com/@serajaris)
