---
title: "llms.txt: делаю блог читаемым для агентов"
date: 2026-03-01
description: "llms.txt — стандарт агент-читаемого веба. Описания в нём — промпты, не SEO-тексты. Кейс sereja.tech: 80+ статей в 8 кластеров за одну сессию."
tags: ["llms-txt", "seo", "ai-agents", "claude-code"]
image: "/images/blog/llms-txt-agent-readable-web-preview.png"
---

llms.txt --- стандарт, объясняющий AI-агентам структуру сайта. Описания в нём --- не SEO-тексты, а промпты: ты программируешь, как агент воспримет твой контент. 844 тысячи сайтов уже добавили llms.txt. Ноль LLM-провайдеров официально подтвердили, что читают. А мой студент скормил блог мультиагентному боту --- и агент назвал контент «золотым».

## Олег, бот и «золотой контент»

Олег --- студент моего [Кружка Вайбкодинга](https://vibecoding.phd/). Он собрал мультиагентного бота и для теста скормил ему sereja.tech. Бот обработал 80+ статей и вернул вердикт: контент «золотой», тематика цельная, всё связано.

Бот не оценивал стиль и заголовки. Он смотрел на структуру: насколько легко извлечь полезное из текста.

Я задумался. Если агенты уже читают блоги --- может, пора адаптировать сайт не только под людей?

## Проблема: веб не читается агентами

Современный веб строится для браузеров. React-приложение рендерит страницу на клиенте --- AI-краулер видит пустой `<div id="root">`. Ни заголовков, ни контента, ни структуры.

У меня Hugo --- статический генератор. HTML готов заранее, JS не нужен. Но даже со статикой проблема остаётся: как агент понимает, что важно на сайте из 80+ страниц? Какие читать первыми, а какие пропустить?

`robots.txt` и `sitemap.xml` проектировались для поисковых ботов. Они говорят «эту страницу можно сканировать», но не говорят «вот зачем эта страница существует». LLM-агенту нужен контекст, не разрешение.

## Стандарт: что такое llms.txt

3 сентября 2024 года [Jeremy Howard](https://jhoward.fastmail.fm/) (Answer.AI, fast.ai) предложил формат [llms.txt](https://llmstxt.org/). Идея: один файл в корне сайта, который объясняет агенту --- что тут есть и зачем.

Формат минимальный. H1 --- название сайта (обязательно). Blockquote --- краткое описание. Дальше H2-секции со ссылками. Опционально --- секция `Optional` для менее важного контента.

```
# Site Name

> Краткое описание сайта

## Тема 1

- [Статья](https://site.com/article/): Описание для агента

## Optional

### Менее важное

- [Ещё статья](https://site.com/more/): Описание
```

По данным [BuiltWith](https://trends.builtwith.com/), llms.txt добавили 844 тысячи сайтов. Среди ранних адоптеров --- Anthropic docs, Cloudflare, Stripe, Vercel, Supabase. По [SE Ranking](https://seranking.com/), 10.13% из проанализированных сайтов имеют llms.txt. При этом в топ-1000 сайтов по трафику --- только 0.3%.

И тут самое интересное. Ни один LLM-провайдер официально не подтвердил, что читает llms.txt. Ноль. Люди добавляют файл на веру.

## Что я сделал на sereja.tech

Я попросил агента создать llms.txt для блога. Не потому что точно знаю, что агенты его читают. А потому что мне самому полезно --- структурировать 80+ статей в читаемый каталог.

### Два файла: lean и full

Агент с маленьким контекстом не осилит полный каталог. Агент с большим --- хочет видеть всё. Я попросил агента сделать две версии:

- **llms.txt** --- lean-версия. 37 ключевых статей, ~3K токенов. Кластеры с hub pages
- **llms-full.txt** --- все 80+ статей, ~5K токенов. Полное описание каждой

Lean-версия ссылается на full, full ссылается на lean. Агент сам выбирает глубину.

### Кластеризация: 8 тем вместо 80 ссылок

80 статей плоским списком --- бесполезно. Даже человеку. Я попросил агента сгруппировать всё в тематические кластеры.

{{< callout type="insight" >}}
**Промпт для Claude Code (Opus 4.6):**

Сгруппируй 80+ статей блога в тематические кластеры (максимум 8-10). Для каждого кластера --- hub page, которая связывает остальные. Формат: H2 + список ссылок.
{{< /callout >}}

Агент выделил 8 кластеров: «Персональная корпорация», «OpenClaw / Clawdbot», «Управление агентом», «Skills, хуки и инфраструктура», «Мультиагентные паттерны», «Пайплайны и автоматизация», «Методология вайбкодинга», «Инструменты и интеграции». Каждый с hub page для навигации.

Тот же принцип единого источника данных, про который писал в [«Один JSON кормит всех»](/blog/data-layer-for-agents/). Только вместо `videos.json` --- `llms.txt`. Один файл, много потребителей.

### Описания как промпты

Самое неочевидное: описания статей в llms.txt --- не SEO-тексты. Агент не кликает по ссылкам ради красивого заголовка. Он парсит описание и решает --- читать или нет.

Я попросил агента переписать все описания:

{{< callout type="insight" >}}
**Промпт для Claude Code (Opus 4.6):**

Перепиши описания статей в llms.txt. Аудитория --- AI-агент, не человек. Для каждой статьи: одно предложение о том, какую конкретную ценность агент может извлечь. Без кликбейта, без абстракций.
{{< /callout >}}

До: «Как настроить Claude Code для максимальной эффективности»
После: «5 приёмов экономии контекста --- 95% контекста жрут результаты тулов. Конкретные приёмы»

Первое --- для человека. Второе --- для агента. Разница: конкретика вместо обещания.

### link tag для discoverability

Чтобы агент мог найти llms.txt с любой страницы, добавил `<link>` в `<head>`:

```html
<link rel="alternate" type="text/plain"
      href="/llms.txt" title="LLM-optimized content">
```

Идея из спецификации llms.txt: агент, попав на любую страницу блога, видит ссылку на агент-читаемую версию. Как RSS-лента, только для LLM.

### Что не получилось

Попросил агента добавить путь к llms.txt в `robots.txt` --- комментарием, как рекомендует спецификация. Не вышло: Hugo при генерации стрипает комментарии. Мелочь, но показательная --- инструменты ещё не адаптировались под молодой стандарт.

## Архитектура: что получилось

```
sereja.tech
│
├── /llms.txt          ← lean: 37 статей, 3K токенов
│   └── ссылка на /llms-full.txt
│
├── /llms-full.txt     ← full: 80+ статей, 5K токенов
│   └── ссылка на /llms.txt
│
├── <head>
│   └── <link rel="alternate" href="/llms.txt">
│
└── 8 кластеров
    ├── Персональная корпорация (5 статей)
    ├── OpenClaw / Clawdbot (4 статьи)
    ├── Управление агентом (11 статей)
    ├── Skills и хуки (8 статей)
    ├── Мультиагентные паттерны (5 статей)
    ├── Пайплайны (5 статей)
    ├── Методология (5 статей)
    └── Инструменты (5 статей)
```

Два уровня глубины. Lean --- для быстрого скана. Full --- для глубокого анализа. Кластеры вместо плоского списка.

## Описания --- это промпты

По-моему, llms.txt --- не каталог и не карта сайта. Это промпт.

Я программирую, как агент воспримет мой контент. «95% контекста жрут результаты тулов» --- не описание статьи. Это утверждение, после которого агент решает: «мне нужно это прочитать».

В [«SEO для блога в эру AI»](/blog/seo-for-ai-era/) я разбирал, как structured data помогает AI-поисковикам понимать контент. llms.txt --- следующий шаг. JSON-LD описывает страницу машиночитаемо. llms.txt описывает весь сайт агенточитаемо.

Эволюция: SEO (оптимизация под поисковики) → AEO (оптимизация под AI-ответы) → AAO (оптимизация под AI-агентов). Каждый следующий уровень требует всё больше контекста и всё меньше маркетинга.

## Парадокс принятия

По данным [Gartner](https://www.gartner.com/), к 2026 году традиционный поисковый трафик упадёт на 25%. AI-ассистенты отвечают на вопросы напрямую --- люди реже переходят на сайты.

Google в ответ добавил собственный llms.txt. И [удалил его в тот же день](https://omnius.so/). Показательно: даже Google не определился с позицией.

844 тысячи сайтов добавили llms.txt. Ноль провайдеров подтвердили, что читают.

Но мне неважно. Упорядочить контент для агентов --- полезно независимо от того, читает ли кто-то файл. Олег скормил блог боту --- бот нашёл структуру. Не из-за llms.txt, а потому что контент был структурирован изначально. llms.txt --- способ сказать об этом явно.

---

## FAQ

### Обязательно ли добавлять llms.txt на сайт?

Нет. Стандарт добровольный, ни один LLM-провайдер не требует его наличия. Но если у вас больше 20 страниц --- llms.txt помогает агентам понять структуру быстрее, чем парсинг всего сайта.

### Чем llms.txt отличается от sitemap.xml?

sitemap.xml говорит краулеру «эти URL существуют». llms.txt говорит агенту «вот зачем эти страницы существуют и какую ценность несут». Sitemap --- список адресов, llms.txt --- карта смысла.

### Нужен ли llms.txt для статического сайта на Hugo?

Hugo генерирует чистый HTML --- агенты и так могут его парсить. Но llms.txt экономит агенту время: вместо парсинга 80 страниц он читает один файл на 3K токенов и понимает, что где.

### Как проверить, что llms.txt работает?

Прямой способ проверить --- скормить URL вашего llms.txt AI-агенту и спросить о вашем сайте. Если ответ точный и структурированный --- файл выполняет свою задачу.
