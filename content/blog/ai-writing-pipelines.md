---
title: "Как устроены конвейеры AI-контента: 12 паттернов"
date: 2026-01-11
description: "Глубокое исследование конвейеров для создания контента с помощью LLM. 12 паттернов, multi-agent архитектуры, human-in-the-loop, content repurposing. Анализ практик Anthropic, Notion, Amazon."
tags:
  - AI
  - LLM
  - content creation
section: "AI"
---

У меня три проекта, где нужно генерировать контент: персональный блог, менторские сессии, когорты школы вайбкодинга.

Каждый проект — свой набор костылей. Захотелось понять, как это делают профессионально.

88 источников, 12 паттернов, много диаграмм. Это то, что я узнал.

## 1. Зачем это исследование

Три проекта, три разных рабочих процесса:

**sereja.tech** — этот блог. Процесс: тема → исследование через Exa → черновик → прогон через deaify (4 критика убирают AI-паттерны) → публикация. Раньше работало медленно — исследование занимало больше времени, чем написание. Но для этой статьи я запустил 4 параллельных research-субагента через Exa — весь сбор 88 источников занял 15 минут вместо нескольких часов.

**mentor** — менторские сессии 1:1. Записываю звонки, транскрибирую, генерирую HTML-страницы с табами: запись, домашка, исследование. Проблема: каждая сессия — одноразовый артефакт. Обсудили React hooks на прошлой неделе, TypeScript сегодня — и эти знания не связаны между собой.

**cohorts** — школа вайбкодинга HSL. Тут самая сложная система: 5-проходная экстракция атомов знаний (инструменты, техники, концепции, workflow, инфраструктура), потом одобрение, генерация с Exa-обогащением, топологическая сортировка по пререквизитам для составления учебного плана.

Что искал:

- Как устроены профессиональные конвейеры создания контента с LLM
- Где в них человек, а где автоматика
- Как переиспользовать контент вместо создания с нуля
- Реальные практики компаний: Anthropic, Notion, Amazon

Что нашёл: 12 паттернов, которые повторяются везде. Удивило, что deaify — это по сути Self-Refine loop, который индустрия формализовала. Мои процессы уже реализуют половину из них, но есть критические пробелы — особенно в переиспользовании контента и параллельной обработке.

## 2. Анатомия LLM Writing Pipeline

### Архитектуры: от линейной к итеративной

Большинство конвейеров генерации контента можно разложить на три базовые архитектуры.

**Линейная (Sequential):** каждый шаг выполняется последовательно, выход одного — вход другого. Проще всего реализовать, но медленно и хрупко: если один шаг сломался — всё встало.

```
Тема → Исследование → План → Черновик → Редактура → Публикация
  │         │           │        │           │            │
  └─────────┴───────────┴────────┴───────────┴────────────┘
                    один поток выполнения
```

**Ветвящаяся (Branching):** независимые задачи выполняются параллельно. Исследование технической документации, примеров кода и лучших практик может идти одновременно — потом оркестратор синтезирует результаты.

```
                    ┌─→ Техническая документация ─┐
                    │                             │
Тема → Оркестратор ─┼─→ Примеры кода ────────────┼─→ Синтез → Черновик
                    │                             │
                    └─→ Лучшие практики ─────────┘
                          параллельное исследование
```

**Итеративная (Iterative):** цикл «генерация → критика → рефайн» повторяется до достижения порога качества или лимита итераций. Это ключевой паттерн для качественного контента.

Исследование Self-Refine (Madaan et al., 2023) показало: итеративный рефайн улучшает качество на ~20% по сравнению с однократной генерацией. Это работает потому, что имитирует человеческий процесс написания — никто не пишет финальный текст с первого раза.

### Multi-Agent паттерны

Вместо одного LLM, который делает всё, индустрия пришла к специализированным агентам. Каждый фокусируется на своей задаче:

| Агент | Ответственность | Пример задачи |
|-------|-----------------|---------------|
| Planner | Структура и план | Разбить тему на секции, определить порядок |
| Researcher | Сбор информации | Найти источники, извлечь факты |
| Writer | Генерация текста | Написать черновик по плану и фактам |
| Critic | Оценка качества | Проверить точность, стиль, полноту |
| Optimizer | Финальная доработка | SEO, форматирование, мета-теги |

Anthropic использует этот паттерн в своей multi-agent research system: ведущий агент координирует параллельных субагентов, каждый исследует свой аспект темы, результаты синтезируются. Если нужно больше информации — динамически спаунятся новые субагенты. Я попробовал этот паттерн в этом исследовании — 4 параллельных research-субагента вместо последовательных. Быстрее примерно в 3 раза.

{{< callout insight >}}
**Паттерн: Orchestrator-Worker**

Оркестратор не делает работу сам — он только координирует. Получает задачу, разбивает на подзадачи, раздаёт воркерам, собирает результаты, решает, нужно ли ещё. Это позволяет масштабировать систему горизонтально.
{{< /callout >}}

### Quality Gates: типы и когда применять

Quality gate — точка в конвейере, где проверяется качество перед переходом к следующему шагу. Без них плохой output ранней стадии портит всё дальше по цепочке.

**Автоматические gates:**

- **Формальные проверки:** длина текста, наличие структуры, валидность ссылок
- **Метрики качества:** perplexity (флюентность), BERTScore (семантическая близость к источникам)
- **Детекция проблем:** галлюцинации, токсичность, off-brand messaging

**LLM-as-Judge gates — другой LLM оценивает output:**

- Выставляет числовую оценку (например, 1-10 по каждому измерению)
- Дешевле человека, но менее надёжно для нюансов

**Human gates:**

- Критические точки: стратегия, верификация фактов, финальное одобрение
- Не всё нужно проверять вручную — только edge cases и высокорисковый контент

Amazon в своей системе автоматической модерации использует conditional gates: если toxicity score попадает в средний диапазон неопределённости — отправляется на человеческий ревью. Низкие значения автоматически одобряются, высокие — автоматически отклоняются.

## 3. Human-in-the-Loop: Где вмешиваться

### Точки принятия решений

Human-in-the-Loop — это не «человек проверяет всё в конце». Это архитектурное решение: где именно в конвейере нужен человек, а где достаточно автоматики.

Исследование из Springer (Timing AI intervention in writing process, 2025) показало парадоксальный результат: позднее вмешательство AI (после того, как человек сам побрейнштормил и набросал структуру) даёт лучшие результаты, чем раннее. Раннее использование AI превращает его в замену мышления, а не инструмент усиления. По моему опыту подтверждается: когда даю AI свой набросок, deaify справляется за 2 итерации. С пустой страницы — обычно 4.

| Фаза | Человек | AI | Почему так |
|------|---------|-----|------------|
| Стратегия | 100% | 0% | Выбор темы, аудитории, цели — нельзя делегировать |
| Исследование | 20% | 80% | AI ищет, человек валидирует источники |
| Планирование | 50% | 50% | AI предлагает структуру, человек корректирует |
| Черновик | 30% | 70% | AI генерирует, человек добавляет опыт и нюансы |
| Редактура | 60% | 40% | AI правит грамматику, человек — смысл и тон |
| Одобрение | 100% | 0% | Финальная ответственность всегда на человеке |

### Batch vs Inline Approval

Два режима человеческого ревью:

**Inline (синхронный):** конвейер останавливается и ждёт одобрения. Используется для:

- Высокорискового контента
- Нового типа контента (первый раз делаем такое)
- Контента с внешними последствиями (публичные заявления, юридические документы)

**Batch (асинхронный):** контент попадает в очередь на ревью, конвейер продолжает работу. Человек периодически просматривает batch и одобряет/отклоняет. Используется для:

- Рутинных вариаций контента
- Локализации
- Социальных постов
- Контента, где задержка некритична

{{< callout insight >}}
**Гибридный паттерн**

AI автоматически помечает элементы, требующие inline review, на основе risk scoring. Остальное идёт в batch queue. Это оптимальный баланс между скоростью и качеством.
{{< /callout >}}

## 4. Content Repurposing с AI

### COPE в эпоху LLM

COPE (Create Once, Publish Everywhere) — принцип, популяризированный NPR ещё до эпохи LLM. Идея: создаёшь контент один раз, публикуешь в разных форматах на разных платформах.

Проблема наивного COPE: копипаста одного текста везде не работает. Блог требует глубины и SEO, Twitter — краткости и хуков, LinkedIn — thought leadership, email — персонализации. Формат меняет суть.

LLM меняют игру: теперь можно не просто конвертировать форматы, а трансформировать контент под требования каждого канала. Один источник → множество адаптированных outputs.

{{< callout warning "Trap: AI сделает из одной статьи 10 постов" >}}
Без качественного контроля это превращается в «slop» — generic AI output, который не несёт ценности. Descript называет это «single-use plastic content». Трансформация требует человеческого ревью на каждом канале.
{{< /callout >}}

### Атомарный контент: что это и как разбивать

Атомарный контент — разбиение на минимальные самодостаточные единицы, которые можно переиспользовать в разных комбинациях.

Иерархия для образовательного контента:

```
Сессия/Запись (исходник)
    │
    ├── Тема 1
    │   ├── Концепция 1.1
    │   │   ├── Пример 1.1.1
    │   │   └── Код 1.1.1
    │   └── Концепция 1.2
    │       └── Пример 1.2.1
    │
    ├── Тема 2
    │   ├── Концепция 2.1
    │   └── Концепция 2.2
    │
    └── Ключевые выводы
        ├── Takeaway 1
        ├── Takeaway 2
        └── Takeaway 3
```

Каждый атом снабжается метаданными:

```json
{
  "atom_id": "react-useEffect-cleanup",
  "type": "concept",
  "title": "Cleanup функции в useEffect",
  "skill_level": "intermediate",
  "prerequisites": ["react-useEffect-basics", "js-closures"],
  "domain": "frontend",
  "estimated_minutes": 15,
  "tags": ["react", "hooks", "memory-leaks"],
  "source_session": "session-2026-01-10",
  "timestamp": "00:23:15-00:38:42",
  "evergreen_score": 0.9
}
```

Метаданные позволяют:

- **Intelligent retrieval:** найти все атомы по теме React hooks для intermediate уровня
- **Prerequisite chains:** автоматически строить последовательность изучения
- **Personalization:** собирать разные learning paths для разных студентов
- **Cross-reference:** связывать атомы из разных сессий

## 5. Как это делают другие

### Anthropic: CLAUDE.md и PR-triggered docs

Anthropic документирует свои подходы в открытых источниках. Ключевые практики:

**Context files (CLAUDE.md):** персистентный контекст, который подгружается в каждую сессию. Содержит стайл-гайды, терминологию, конвенции репозитория. Это решает проблему «каждый раз объяснять одно и то же».

**Dual workflow для документации:**

- **PR-triggered:** при изменении кода автоматически анализируется diff → идентифицируются затронутые доки → генерируются обновления → человеческий ревью
- **Scheduled maintenance:** ежедневный обзор изменений за 24 часа → консолидированные обновления документации

### Notion: датасеты, три типа оценки

Notion описал свой процесс разработки AI-фич в интервью с Braintrust. Эволюция:

**Было:** JSONL-файлы в Git, дорогая ручная оценка человеком, медленные итерации.

**Стало:** Версионированные датасеты из реального использования, три метода оценки (heuristic rules, LLM-as-judge, human review), experiment-driven development — гипотеза → эксперимент → измерение → решение.

### Indie creators: 6-agent pipeline, 720 статей в месяц

Vicky.dev описывает полностью автоматизированную систему:

**Архитектура:**

1. GNews API → получение свежих новостей
2. Google Sheets → проверка дубликатов
3. GPT-4o-mini → генерация статьи
4. WordPress REST API → публикация

Цикл каждые 2 минуты. Результат: 720+ статей в месяц. Задача, занимавшая 20-30 часов в неделю, свелась к мониторингу.

### Реалистичные ожидания: 1.5x, не 10x

Tom Johnson (I'd Rather Be Writing) в своих предсказаниях на 2026 год пишет:

> «AI productivity boost has stabilized at ~1.5x factor. Validation overhead limits velocity gains.»

Это критически важно. Маркетинговые обещания «10x productivity» не выдерживают проверки реальностью. Причины:

- **Validation overhead:** каждый AI output требует проверки
- **Hallucination risk:** факты нужно верифицировать
- **Brand alignment:** tone и voice требуют ручной подстройки
- **Edge cases:** нетривиальные ситуации всё ещё требуют человека

{{< callout insight >}}
**Правильная ментальная модель**

AI не заменяет людей, а усиливает их. Content creator становится «AI-human workflow architect» — тот, кто проектирует процессы, а не пишет каждое слово сам.
{{< /callout >}}

## 6. Анализ моих текущих workflows

### sereja.tech: что хорошо, что добавить

**Текущая архитектура:**

```
Тема → Exa research → Черновик HTML → Deaify (4 критика) → Публикация

Deaify critics:
  • generic-critic (общие AI-паттерны)
  • rhythm-critic (однообразие ритма)
  • specifics-critic (отсутствие конкретики)
  • fact-checker (верификация фактов)
```

**Что работает хорошо:**

- Self-refine loop через deaify — правильный паттерн
- Специализированные критики вместо одного generic
- Research → Draft → Critique → Refine flow соответствует индустриальным практикам

**Конкретные рекомендации:**

1. **Quality scoring в deaify** (высокий приоритет, 2-4 часа)
   - Каждый критик выдаёт AI-ness score (0-100)
   - Exit condition: все scores < 20 ИЛИ max 3 итерации
   - Предотвращает бесконечный рефайн и даёт метрику качества

2. **Параллельные research субагенты** (высокий приоритет, 8-12 часов)
   - Три параллельных субагента: TechnicalDocs, Examples, BestPractices
   - Оркестратор синтезирует результаты
   - Ускоряет исследование в 2-3 раза

### mentor: критический gap — session atomization

**Критическая проблема:** сессии — одноразовые артефакты. Обсудили React hooks на прошлой неделе, TypeScript сегодня — и эти знания не связаны. Каждая сессия существует изолированно.

Это прямое нарушение принципа COPE. Контент создаётся, но не переиспользуется.

### cohorts: золотой стандарт, добавить multi-channel

**Что работает отлично:**

- Атомарная архитектура — именно то, что рекомендует индустрия
- 5-pass extraction создаёт хорошо категоризированные атомы
- Топологическая сортировка по пререквизитам — правильный подход для adaptive learning paths
- Approval gates предотвращают попадание низкокачественных атомов в curriculum

{{< callout insight >}}
**Key Takeaway**

Текущие workflows уже реализуют половину индустриальных паттернов. Главные gaps: параллельная обработка (ускорение), атомизация контента (переиспользование), автоматический scoring (масштабирование). Не нужно переписывать с нуля — нужно точечные улучшения.
{{< /callout >}}

---

## Источники

### LLM Pipelines и архитектуры

- [Chain complex prompts for stronger performance — Anthropic](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/chain-prompts) — prompt chaining разбивает сложные задачи на последовательные подзадачи
- [Building Generative AI prompt chaining workflows — AWS](https://aws.amazon.com/blogs/machine-learning/building-generative-ai-prompt-chaining-workflows-with-human-in-the-loop) — параллелизация, специализация моделей, HITL
- [How we built our multi-agent research system — Anthropic](https://www.anthropic.com/engineering/multi-agent-research-system) — orchestrator-worker паттерн с параллельными субагентами
- [Self-Refine: Iterative Refinement with Self-Feedback — arXiv](https://arxiv.org/abs/2303.17651) — generate → feedback → refine loop, ~20% improvement

### Human-in-the-Loop

- [Why AI still needs you: Exploring HITL systems — WorkOS](https://workos.com/blog/why-ai-still-needs-you-exploring-human-in-the-loop-systems) — HITL как архитектурное решение
- [AI-written critiques help humans notice flaws — OpenAI](https://openai.com/research/critiques) — AI-assisted critique +65% обнаружения проблем
- [Build high quality AI features with simple feedback loops — Dovetail](https://dovetail.com/blog/build-high-quality-ai-features-with-simple-feedback-loops/) — snapshot evals, prompts как unit tests

### Content Repurposing

- [COPE — Create Once, Publish Everywhere — Annertech](https://annertech.com/blog/cope-create-once-publish-everywhere) — философия COPE от NPR
- [The slop-free guide to AI content repurposing — Descript](https://www.descript.com/blog/article/ai-content-repurposing) — избежать generic AI output
- [Why your brand needs modular content — Contentful](https://www.contentful.com/blog/modular-content/) — modular = reusable = scalable

### Industry Practices

- [How Notion develops world-class AI features — Braintrust](https://braintrust.dev/blog/notion) — versioned datasets, heuristic/LLM/human evaluation
- [HALF-Eval Framework — Amazon Science](https://assets.amazon.science/4a/de/85b797d4482a80bcbc9d6151167a/human-aligned-long-form-evaluation-half-eval-framework-for-assessing-ai-generated-content-and-improvement.pdf) — checklist → ML aggregation для holistic scoring
- [12 predictions for tech comm in 2026 — Tom Johnson](https://idratherbewriting.com/blog/tech-comm-predictions-for-2026) — 1.5x productivity, не 10x
