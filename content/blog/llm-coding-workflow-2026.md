---
title: "Как работать с LLM в 2026: workflow Addy Osmani"
date: 2026-01-11
description: "Сборник практик для работы с LLM в 2026: планирование до кода, context engineering, git worktrees для параллельных агентов, смена моделей. Источники: Addy Osmani, O'Reilly, incident.io."
tags:
  - LLM
  - AI
  - workflow
section: "AI"
---

Addy Osmani из Google опубликовал свой подход к работе с AI-ассистентами.

Собрал его инсайты плюс практики из других источников — O'Reilly, incident.io, Builder.io.

Получился сборник того, что реально работает в 2026.

## Планируй до кода

"Waterfall за 15 минут" — так Osmani называет подход. Перед первой строкой кода пишешь документ: что делаем, что не делаем, какие edge cases. Harper Reed идёт дальше — у него два файла: `spec.md` с требованиями и `plan.md` с пошаговым планом.

Сначала обсуждаешь задачу с AI как с архитектором — уточняешь требования, проверяешь понимание. Потом код. Без этого модель генерирует "в молоко".

В Claude Code — Plan Mode (Shift+Tab дважды). Cursor выполняет промпты из файла последовательно. Думай до того, как пишешь.

## Декомпозиция: один запрос = одна задача

Одна функция, один баг, одна фича за промпт. Не больше. Osmani называет это "iterative chunks".

Большие запросы ломают модель — она путается, дублирует код, забывает начало промпта. Порог где-то на 150-200 словах. После него качество падает резко.

Практический паттерн: разбей задачу на 5-7 шагов, каждый шаг — отдельный промпт. Между промптами проверяй результат. Если что-то пошло не так — откатывай и переформулируй, пока контекст не засорился.

## Context engineering

Термин из отчёта O'Reilly Radar 2026: context engineering — это управление тем, что модель знает о проекте. Prompt engineering — как спрашиваешь, context engineering — что модель видит.

Инструменты для упаковки контекста: `gitingest`, `repo2txt`, Context7. Они дампят нужные части кодовой базы в формат, который AI может переварить. Модель не знает твою архитектуру — покажи ей.

Но контекст надо фильтровать. Cursor делает это через локальный индекс с semantic search. Windsurf использует "Fast Context" для выборки по сотням файлов. Общий принцип: релевантный контекст улучшает ответы, нерелевантный — размывает.

CLAUDE.md — файл с правилами проекта. Стиль кода, конвенции, запреты. В Copilot есть custom instructions. Cursor поддерживает .cursorrules. Смысл один: формализуй то, что модель должна знать всегда.

## Смена моделей

"Model musical chairs" — застрял на Claude, переключись на Gemini. Свежий взгляд часто ломает затык.

Claude — мой выбор для сложной архитектуры и логики. GPT-5.2 быстрее справляется с рутиной: бойлерплейт, миграции, однотипные тесты. Gemini 3 Flash (декабрь 2025) работает арбитром — быстрый и дешёвый. Когда нужно решать, а не генерировать — GPT-5 в режиме thinking.

Кросс-ревью: GPT критикует код Claude. На прошлой неделе GPT нашёл race condition в async-хендлере, который Claude пропустил. Дешевле, чем баг в проде.

## Git worktrees для параллельных агентов

Практика, которая взорвала 2025-й. Git worktrees держат несколько веток в разных папках одновременно — каждый агент в своей песочнице.

Incident.io запускает 4-5 Claude-агентов параллельно, каждый в своём worktree. Workmux от raine.dev связывает worktrees с tmux — получается дашборд: видишь все задачи, переключаешься между агентами в одном терминале.

Зачем это нужно: агенты не мешают друг другу. Один может сломать типы в своей ветке — это не аффектит остальных. Можно параллельно пробовать три архитектурных подхода и потом сравнить.

Базовый workflow: главный агент генерирует список задач, потом `workmux add` создаёт worktree для каждой. Агенты работают параллельно, ты ревьюишь и мержишь через `workmux merge --rebase`.

## Субагенты для изоляции контекста

Другой подход к параллелизму — субагенты. В Claude Code это Task tool с разными типами агентов: Debugger Agent, Security Agent, Documentation Agent.

Преимущество: субагент работает в чистом контексте. Основная сессия не засоряется. После завершения субагент возвращает результат, а его внутренний контекст выбрасывается.

Когда использовать: задача самодостаточная и не требует истории разговора. Хочешь сэкономить на токенах — запусти на Haiku. Контекст основной сессии забит — делегируй.

## Git-дисциплина

Коммить после каждой мини-задачи. Это точки сохранения для отката. Osmani формулирует жёстко: "Never commit code you can't explain."

AI генерирует быстро — легко накопить гору изменений и потерять контроль. Частые коммиты держат историю читаемой. Если что-то сломалось — откат на один коммит назад, а не раскопки в 500-строчном диффе.

## Senior-скиллы решают

O'Reilly Radar 2026: роль разработчика сместилась от синтаксиса к архитектуре и ревью. Эпоха "10x engineer" закончилась — началась "100x organization".

AI усиливает экспертизу. Архитектура, trade-offs, управление сложностью — это твоё. Модель быстрее генерирует, значит строже дисциплина. Тесты, ревью, стандарты — важнее, чем раньше.

{{< callout insight >}}
**Главный вывод**

Относись к AI как к джуниору с суперсилой скорости. Проверяй всё. Планируй до кода. Коммить часто. И помни: ты остаёшься ответственным инженером.
{{< /callout >}}

---

## Источники

- [My LLM coding workflow going into 2026 — Addy Osmani](https://addyo.substack.com/p/my-llm-coding-workflow-going-into)
- [Signals for 2026 — O'Reilly Radar](https://www.oreilly.com/radar/signals-for-2026/)
- [Shipping faster with Claude Code and Git Worktrees — incident.io](https://incident.io/blog/shipping-faster-with-claude-code-and-git-worktrees)
- [Using git worktrees to parallelize AI coding — raine.dev](https://raine.dev/blog/git-worktrees-parallel-agents/)
- [My LLM codegen workflow — Harper Reed](https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/)
- [Coding with LLMs in 2026: Strategy — ThinkPeak](https://thinkpeak.ai/coding-with-llms-2026-strategy/)
- [How I use Claude Code — Builder.io](https://www.builder.io/blog/claude-code)
- [The State of AI Pair Programming in 2026 — DayZero](https://www.dayzero.live/software-engineering/the-state-of-ai-pair-programming-in-2026)
